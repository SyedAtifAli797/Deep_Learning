{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulIhivoemhmY"
      },
      "source": [
        "## **Scratch Implementation of CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uMVULG0SDc39"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy import signal\n",
        "# signal module from scipy performs operations on 2D matrices (typically used for image processing, filtering, and feature extraction)\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juC7pcDPEQ-Q"
      },
      "source": [
        "### Base layer class to specify the layer properites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8ZVmLggKDph9"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        pass\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysoDlq1OEYdG"
      },
      "source": [
        "### Forward and Backward Propagation in Convolution Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GMtbS1raEe_X"
      },
      "outputs": [],
      "source": [
        "class Convolutional(Layer):\n",
        "\n",
        "   def __init__(self, input_shape, kernel_size, depth):\n",
        "\n",
        "        # Input_shape is 3 dimensional (dxhxw)\n",
        "        # input_depth  = no.of channels\n",
        "        # input_height = image height and\n",
        "        # input_width  = image width\n",
        "        input_depth, input_height, input_width = input_shape\n",
        "\n",
        "        # Depth represents the number of kernels of our convolutional layer\n",
        "        self.depth = depth\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "        # number of channels in the image\n",
        "        self.input_depth = input_depth\n",
        "\n",
        "        # Calculating Conv layer output of 3 dimensions\n",
        "        # first dim  = number of filters/kernels\n",
        "        # second dim = height of the output matrix after applying convolution\n",
        "        # third dim  = width of the output matrix after applying convolution\n",
        "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
        "\n",
        "        # Kernels shape takes 4 dimensions\n",
        "        # depth = no. of kernels\n",
        "        # input_depth = image channels\n",
        "        # kernel_size = kernel dimension\n",
        "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
        "\n",
        "        # Initalizing the Kernels weights randomly\n",
        "        self.kernels = np.random.randn(*self.kernels_shape)\n",
        "\n",
        "        # Initializing the biases randomly\n",
        "        self.biases = np.random.rand(*self.output_shape)\n",
        "\n",
        "    # Forward pass\n",
        "   def forward(self, input):\n",
        "        self.input = input\n",
        "        # Inititialize output matrix with output_shape\n",
        "        self.output = np.zeros(self.output_shape)\n",
        "\n",
        "        # Nested loop for traversing across all filters (depth), then all channels (input_depth) in every input image\n",
        "        for i in range(self.depth):\n",
        "            for j in range(self.input_depth):\n",
        "                # Output = Conv(Input, Kernel) + Bias\n",
        "                self.output[i] = self.biases[i] + signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")    # valid stands for no padding\n",
        "        return self.output\n",
        "\n",
        "   def backward(self, output_gradient, learning_rate):\n",
        "        # Intializing the gradient of the kernels as zeros\n",
        "        kernels_gradient = np.zeros(self.kernels_shape)\n",
        "\n",
        "        # Intializing the gradient of the input as zeros\n",
        "        input_gradient = np.zeros(self.input_shape)\n",
        "\n",
        "        # Nested loop for updating the gradients of kernels and inputs,\n",
        "        # first traversing all filters (depth), then all channels (input_depth) in every input image\n",
        "        for i in range(self.depth):\n",
        "            for j in range(self.input_depth):\n",
        "                # Calculate kernels gradient in every i and j index in the kernel,\n",
        "                kernels_gradient[i,j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")  # Computes the cross-correlation between two 2D arrays\n",
        "\n",
        "                # Calculate input gradient by sliding the kernel on the output gradient matrix\n",
        "                input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i, j], \"full\")  # Performs 2D convolution but flips the kernel before sliding over the input\n",
        "                                                                                                        # full stands for full padding\n",
        "                                                                                                        # padding = kernel size−1\n",
        "\n",
        "        # Update the kernels and biases w.r.t. learned features (stored in gradients)\n",
        "        self.kernels -= learning_rate * kernels_gradient\n",
        "        self.biases -= learning_rate * np.sum(output_gradient)\n",
        "\n",
        "        return input_gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMc0wTyZEpmN"
      },
      "source": [
        "### Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f9Ofv4hKEkbm"
      },
      "outputs": [],
      "source": [
        "# Base Activation class to specify the default properties of the Activation Layer\n",
        "\n",
        "class Activation(Layer):\n",
        "\n",
        "    def __init__(self, activation, derivative_activation):\n",
        "        self.activation = activation\n",
        "        self.derivative_activation = derivative_activation\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return self.activation(self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return np.multiply(output_gradient, self.derivative_activation(self.input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n6UWHR0JEm75"
      },
      "outputs": [],
      "source": [
        "class ReLU(Activation):\n",
        "  def __init__(self):\n",
        "    def relu(x):\n",
        "      return np.where(x>0, x, 0)\n",
        "\n",
        "    def derivative_relu(x):\n",
        "      return np.where(x>0, 1, 0)\n",
        "\n",
        "    super().__init__(relu, derivative_relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z5SM7f1cEr3K"
      },
      "outputs": [],
      "source": [
        "class TanH(Activation):\n",
        "    def __init__(self):\n",
        "      def tanh(x):\n",
        "        tanH = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "        return tanH\n",
        "        # return np.tanh(x)        # built-in function using numpy\n",
        "\n",
        "      def derivative_tanh(x):\n",
        "        return 1 - ((np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)))**2\n",
        "        # return 1 - np.tanh(x) ** 2\n",
        "\n",
        "      super().__init__(tanh, derivative_tanh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WoxiIgv7Euku"
      },
      "outputs": [],
      "source": [
        "class Softmax(Layer):\n",
        "\n",
        "    def forward(self, input):\n",
        "      exp_z = np.exp(input)\n",
        "      self.output = exp_z/np.sum(exp_z, axis=0)\n",
        "      return self.output\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "      n = np.size(self.output)\n",
        "      return np.dot((np.identity(n) - self.output.T) * self.output, output_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQzdO6FzE1Cx"
      },
      "source": [
        "### Max Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9Xi3336OEw2W"
      },
      "outputs": [],
      "source": [
        "class MaxPool(Layer):\n",
        "\n",
        "    def __init__(self, input_shape, kernel_size, depth, stride):\n",
        "        input_depth, input_height, input_width = input_shape\n",
        "        self.input_shape = input_shape\n",
        "        self.kernel_size = kernel_size\n",
        "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
        "        self.depth = depth\n",
        "        self.input_depth = input_depth\n",
        "        self.kernels = np.random.randn(*self.kernels_shape)\n",
        "        self.stride = stride\n",
        "        self.input_height, self.input_width = input_height, input_width\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        KH = ((self.input_height-self.kernel_size)//self.stride) + 1\n",
        "        KW = ((self.input_width-self.kernel_size)//self.stride) + 1\n",
        "        self.output = np.zeros((self.input_depth, KH, KW))\n",
        "\n",
        "        for depth in range(self.input_depth):\n",
        "            for r in range(0, self.input_height-1, self.stride):\n",
        "                for c in range(0, self.input_width-1, self.stride):\n",
        "                    self.output[depth, r//self.stride, c//self.stride] = np.max(self.input[depth,\n",
        "                                                                                r:r+self.kernel_size,\n",
        "                                                                                c:c+self.kernel_size])\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        self.output_gradient = np.zeros(self.input_shape)\n",
        "        dx = np.zeros(self.input_shape)\n",
        "        for depth in range(self.input_depth):\n",
        "            for r in range(0, self.input_height-1, self.stride):\n",
        "                for c in range(0, self.input_width-1, self.stride):\n",
        "                    grad_pool = self.output[depth, r*self.stride:r*self.stride+self.kernel_size, c*self.stride:c*self.stride+self.kernel_size]\n",
        "                    mask = (grad_pool == np.max(grad_pool))\n",
        "                    dx[depth, r*self.stride:r*self.stride+self.kernel_size, c*self.stride : c*self.stride+self.kernel_size] = mask*self.output_gradient[depth, r, c]\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRdiZeIBkOGl"
      },
      "source": [
        "### Reshaping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "htTuKSJkE0HL"
      },
      "outputs": [],
      "source": [
        "class Reshape(Layer):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        return np.reshape(input, self.output_shape)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return np.reshape(output_gradient, self.input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQtfhRY8kS2n"
      },
      "source": [
        "### Fully Connected Dense Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fq1ymXJsE6mK"
      },
      "outputs": [],
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        # Defining shape of weights matrix\n",
        "        self.weights = np.random.randn(output_size, input_size)\n",
        "        # Defining shape of bias matrix\n",
        "        self.bias = np.random.randn(output_size, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.dot(self.weights, self.input) + self.bias #X.W+b\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        # Calculate weights gradient by dot product of output gradient and transpose of input\n",
        "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
        "\n",
        "        # Calculating the input gradient by performing dot product of weights transpose and output gradient\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "\n",
        "        # Updating the weights of the layer\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "\n",
        "        # Updating the bias of the layer\n",
        "        self.bias -= learning_rate * output_gradient\n",
        "\n",
        "        return input_gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVkpxx4HkWV0"
      },
      "source": [
        "### Defining the Loss Function (for binary classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YthcHREXE9Lv"
      },
      "outputs": [],
      "source": [
        "def log_loss(y_true, y_pred):\n",
        "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def derivative_log_loss(y_true, y_pred):\n",
        "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWpE3o7d1TUO"
      },
      "source": [
        "## **Loading the Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHHzYj8AkePf"
      },
      "source": [
        "**Load MNIST Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilxH8dN7E_Kh",
        "outputId": "b69951dd-334e-4521-bd4a-ef90ef90f19d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZe3lj01FBi-",
        "outputId": "55169a8a-27a1-47ba-80eb-8ef88f453bc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p93JLfPpFEzX",
        "outputId": "d1b18d65-d4ce-41a5-954e-605eaf058fab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 0], dtype=uint8)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBcNVlloFKgV",
        "outputId": "5719a761-2408-4950-861b-2fd7f64e5302"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq4nrG6SFS2A"
      },
      "source": [
        "#### For simplicity we select only 2000 images from class 0 and 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F0AzsLllVH0"
      },
      "source": [
        "Function to preprocess the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dzfOYCIyFMhg"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(x, y, limit):\n",
        "\n",
        "    zero_index = np.where(y == 0)[0][:limit]\n",
        "    one_index = np.where(y == 1)[0][:limit]\n",
        "\n",
        "    all_indices = np.hstack((zero_index, one_index))\n",
        "    all_indices = np.random.permutation(all_indices)\n",
        "    x, y = x[all_indices], y[all_indices]\n",
        "\n",
        "    # Reshaping the data\n",
        "    x = x.reshape(len(x), 1, 28, 28)\n",
        "    # Normalize all pixel values [between 0-1],\n",
        "    x = x.astype(\"float32\") / 255\n",
        "\n",
        "    # One hot encode all the labels\n",
        "    y = to_categorical(y)\n",
        "    y = y.reshape(len(y), 2, 1)\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jXV9ZatQXAUZ"
      },
      "outputs": [],
      "source": [
        "limit = 5000\n",
        "X_train, y_train = preprocess_data(X_train, y_train, limit)\n",
        "X_test, y_test = preprocess_data(X_test, y_test, limit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHopAS1TFWdn",
        "outputId": "89a2114c-6f8e-47ea-e182-e958530984d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((10000, 1, 28, 28), (10000, 2, 1))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn83F_tEXXen",
        "outputId": "9a28a3d3-a4a1-4ba9-e45f-8310a2054678"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((2115, 1, 28, 28), (2115, 2, 1))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7D2zXcJJqLb"
      },
      "source": [
        "### Defining the network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zrVjnaOcG4w2"
      },
      "outputs": [],
      "source": [
        "network = [\n",
        "    #input_shape, kernel_size, depth/filter\n",
        "    Convolutional((1, 28, 28), 3, 5),\n",
        "    ReLU(),\n",
        "\n",
        "    #input_shape, kernel_size, depth, stride\n",
        "    MaxPool((5,26,26), 2, 5, 1),\n",
        "\n",
        "    #input_shape, output_shape\n",
        "    Reshape((5, 25, 25), (5 * 25 * 25, 1)),\n",
        "\n",
        "    #input_size, output_size\n",
        "    Dense(5 * 25 * 25, 100),\n",
        "    TanH(),\n",
        "\n",
        "    #input_size, output_size\n",
        "    Dense(100, 2), #2classes\n",
        "    Softmax()\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdWHuDN1TZSm",
        "outputId": "c801f842-e112-43f4-e20d-7ecea9702a8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<__main__.Convolutional at 0x7b36aafdc110>,\n",
              " <__main__.ReLU at 0x7b3697b556d0>,\n",
              " <__main__.MaxPool at 0x7b3697b68b10>,\n",
              " <__main__.Reshape at 0x7b3697b68890>,\n",
              " <__main__.Dense at 0x7b3697b68850>,\n",
              " <__main__.TanH at 0x7b36978a4c10>,\n",
              " <__main__.Dense at 0x7b3697921f10>,\n",
              " <__main__.Softmax at 0x7b3697922b10>]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxQQ1S3QkoPl"
      },
      "source": [
        "### Defining the train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "H8QViSicTbWa"
      },
      "outputs": [],
      "source": [
        "def train(network, loss, loss_derivative, x_train, y_train, epochs = 5, learning_rate = 0.01):\n",
        "    for e in range(epochs):\n",
        "        print('Epoch Start:',e)\n",
        "        error = 0\n",
        "        idx = 0\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            if (idx+1) % 500 == 0:\n",
        "              print(f\"Epoch {e}: {idx+1}/{len(y_train)}\")\n",
        "\n",
        "            idx += 1\n",
        "\n",
        "            # Forward pass to predict on the training data\n",
        "            output = predict(network, x)\n",
        "\n",
        "            # Summing the losses to optimize the network's weights and biases\n",
        "            error += loss(y, output)\n",
        "\n",
        "            # Perform backward pass through every layer\n",
        "            grad = loss_derivative(y, output)\n",
        "            for layer in reversed(network):\n",
        "                grad = layer.backward(grad, learning_rate)\n",
        "\n",
        "        error /= len(x_train)\n",
        "        print(f\"Epoch : {e + 1}/{epochs}, loss = {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjk9T4BKmJXr"
      },
      "source": [
        "Function to make a prediction on a given input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bL4CTnaKmKP4"
      },
      "outputs": [],
      "source": [
        "def predict(network, input):\n",
        "    output = input\n",
        "    for layer in network:\n",
        "        output = layer.forward(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJN4Go4MkF4v"
      },
      "source": [
        "### Fitting the model to the data by calling the train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnZhAQzkT-S4",
        "outputId": "59061e47-89fd-43f0-e831-fae52617cd7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch Start: 0\n",
            "Epoch 0: 500/10000\n",
            "Epoch 0: 1000/10000\n",
            "Epoch 0: 1500/10000\n",
            "Epoch 0: 2000/10000\n",
            "Epoch 0: 2500/10000\n",
            "Epoch 0: 3000/10000\n",
            "Epoch 0: 3500/10000\n",
            "Epoch 0: 4000/10000\n",
            "Epoch 0: 4500/10000\n",
            "Epoch 0: 5000/10000\n",
            "Epoch 0: 5500/10000\n",
            "Epoch 0: 6000/10000\n",
            "Epoch 0: 6500/10000\n",
            "Epoch 0: 7000/10000\n",
            "Epoch 0: 7500/10000\n",
            "Epoch 0: 8000/10000\n",
            "Epoch 0: 8500/10000\n",
            "Epoch 0: 9000/10000\n",
            "Epoch 0: 9500/10000\n",
            "Epoch 0: 10000/10000\n",
            "Epoch : 1/5, loss = 0.052473486605098436\n",
            "Epoch Start: 1\n",
            "Epoch 1: 500/10000\n",
            "Epoch 1: 1000/10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-c0b49f469f48>:2: RuntimeWarning: divide by zero encountered in log\n",
            "  return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
            "<ipython-input-11-c0b49f469f48>:2: RuntimeWarning: invalid value encountered in multiply\n",
            "  return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
            "<ipython-input-11-c0b49f469f48>:5: RuntimeWarning: invalid value encountered in divide\n",
            "  return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: 1500/10000\n",
            "Epoch 1: 2000/10000\n",
            "Epoch 1: 2500/10000\n",
            "Epoch 1: 3000/10000\n",
            "Epoch 1: 3500/10000\n",
            "Epoch 1: 4000/10000\n",
            "Epoch 1: 4500/10000\n",
            "Epoch 1: 5000/10000\n",
            "Epoch 1: 5500/10000\n",
            "Epoch 1: 6000/10000\n",
            "Epoch 1: 6500/10000\n",
            "Epoch 1: 7000/10000\n",
            "Epoch 1: 7500/10000\n",
            "Epoch 1: 8000/10000\n",
            "Epoch 1: 8500/10000\n",
            "Epoch 1: 9000/10000\n",
            "Epoch 1: 9500/10000\n",
            "Epoch 1: 10000/10000\n",
            "Epoch : 2/5, loss = nan\n",
            "Epoch Start: 2\n",
            "Epoch 2: 500/10000\n",
            "Epoch 2: 1000/10000\n",
            "Epoch 2: 1500/10000\n",
            "Epoch 2: 2000/10000\n",
            "Epoch 2: 2500/10000\n",
            "Epoch 2: 3000/10000\n",
            "Epoch 2: 3500/10000\n",
            "Epoch 2: 4000/10000\n",
            "Epoch 2: 4500/10000\n",
            "Epoch 2: 5000/10000\n",
            "Epoch 2: 5500/10000\n",
            "Epoch 2: 6000/10000\n",
            "Epoch 2: 6500/10000\n",
            "Epoch 2: 7000/10000\n",
            "Epoch 2: 7500/10000\n",
            "Epoch 2: 8000/10000\n",
            "Epoch 2: 8500/10000\n",
            "Epoch 2: 9000/10000\n",
            "Epoch 2: 9500/10000\n",
            "Epoch 2: 10000/10000\n",
            "Epoch : 3/5, loss = nan\n",
            "Epoch Start: 3\n",
            "Epoch 3: 500/10000\n",
            "Epoch 3: 1000/10000\n",
            "Epoch 3: 1500/10000\n",
            "Epoch 3: 2000/10000\n",
            "Epoch 3: 2500/10000\n",
            "Epoch 3: 3000/10000\n",
            "Epoch 3: 3500/10000\n",
            "Epoch 3: 4000/10000\n",
            "Epoch 3: 4500/10000\n",
            "Epoch 3: 5000/10000\n",
            "Epoch 3: 5500/10000\n",
            "Epoch 3: 6000/10000\n",
            "Epoch 3: 6500/10000\n",
            "Epoch 3: 7000/10000\n",
            "Epoch 3: 7500/10000\n",
            "Epoch 3: 8000/10000\n",
            "Epoch 3: 8500/10000\n",
            "Epoch 3: 9000/10000\n",
            "Epoch 3: 9500/10000\n",
            "Epoch 3: 10000/10000\n",
            "Epoch : 4/5, loss = nan\n",
            "Epoch Start: 4\n",
            "Epoch 4: 500/10000\n",
            "Epoch 4: 1000/10000\n",
            "Epoch 4: 1500/10000\n",
            "Epoch 4: 2000/10000\n",
            "Epoch 4: 2500/10000\n",
            "Epoch 4: 3000/10000\n",
            "Epoch 4: 3500/10000\n",
            "Epoch 4: 4000/10000\n",
            "Epoch 4: 4500/10000\n",
            "Epoch 4: 5000/10000\n",
            "Epoch 4: 5500/10000\n",
            "Epoch 4: 6000/10000\n",
            "Epoch 4: 6500/10000\n",
            "Epoch 4: 7000/10000\n",
            "Epoch 4: 7500/10000\n",
            "Epoch 4: 8000/10000\n",
            "Epoch 4: 8500/10000\n",
            "Epoch 4: 9000/10000\n",
            "Epoch 4: 9500/10000\n",
            "Epoch 4: 10000/10000\n",
            "Epoch : 5/5, loss = nan\n"
          ]
        }
      ],
      "source": [
        "train(\n",
        "    network,\n",
        "    log_loss,\n",
        "    derivative_log_loss,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs = 5,\n",
        "    learning_rate = 0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Dz9_UuZBmy"
      },
      "source": [
        "### Function to calculate Accuracy on the Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iix1lB5IWh6K",
        "outputId": "b5ef99f8-1479-49be-e357-25e30c531da9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2115"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEPeMywMUBoZ",
        "outputId": "69bad98d-dae7-4086-9429-f10e4d5fd166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the Network on Test data is 46.335697399527184 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "for x, y in zip(X_test, y_test):\n",
        "    output = predict(network, x)\n",
        "    if np.equal(np.argmax(output), np.argmax(y)):\n",
        "       correct += 1\n",
        "\n",
        "print(f\"Accuracy of the Network on Test data is {(correct/len(X_test)) * 100} %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko_vE3m9WN9B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
